---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "README-"
)
```

# famospypy

famospypy provides an automated and unbiased model selection algorithm that aims at determining the most appropriate subset of model parameters to describe a specific data set. Due to its flexibility with respect to the cost/optimisation function, famospy can handle many different mathematical structures, including for example regression models and ODEs.

## Installation

You can install famospypy from github using pip:

```{python inits, eval = FALSE}
pip install git+git://github.com/GabelHub/famospy_py.git
```

## Features

### Adaptive methods of model selection

famospypy uses three different methods to find appropriate models to test:

- Forward search: Add a parameter to the currently best model
- Backward elimination: Remove a parameter from the currently best model
- Swap search: Replace a parameter of the currently best model by another. The parameters that can be swapped need to be specified by the user

famospy keeps track of the methods used in the previous iterations and dynamically changes them according to the outcome of each iteration.

### Flexibility

famospy is designed to allow for a maximum of flexibility regarding the fitting procedures and model types in R. While it comes with the default option to fit the cost function via _scipy.minimize_, it also allows the users to specify their own optimisation routines.

### Easy parallelisation

famospy allows for easy parallelisation, meaning many different models can be tested simultaneously if the required computational resources are available. 

### Smart testing procedures

famospy keeps track of previously tested models and checks also that each model fulfills all user-specified restrictions, therefore testing only relevant models and saving computational resources.

## Example

As a simple example, we generate a simple data set generated by two parameters and apply the famospy on a global model consisting of five different parameters.

```{r example2, eval = FALSE}
import math
import numpy as np
import tempfile
import famospy

#set parameter values
truep2 = 3
truep5 = 2

#generate simulated data
simDataX = np.array([i for i in range(0,10)])
simDataY = np.array([truep2**2 * x**2 - math.exp(truep5 * x) for x in simDataX])

#set initial parameter values
inits = dict(p1 = 3, p2 = 4, p3 = -2, p4 = 2, p5 = 0)

#define cost function
def cost_function(parms, binary,simX, simY):
  #calculate the least squares for the current parameter set
  res = np.array([4*parms["p1"] + parms["p2"]**2 * x**2 + parms["p3"]*math.sin(x) + parms["p4"]*x - math.exp(parms["p5"] * x) for x in simX])
  diff = np.sum((res - simY)**2)
  #calculate the AICc
  nrPar = len([1 for i in binary if i == 1])
  nrData = len(simDataX)
  aicc = diff + 2*nrPar + 2*nrPar*(nrPar + 1)/(nrData - nrPar - 1)
  return(aicc)

#define swap parameter set
swaps = [["p1","p5"]]

#use a temporary directory
tmp = tempfile.TemporaryDirectory()

#run famospy
if __name__ == "__main__":
  out = famospy.famos(initPar = inits,
              fitFn = cost_function,
              homedir = tmp.name,
              method = "swap",
              parallelise = True,
              swapParameters = swaps,
              initModelType = ["p1", "p3"],
              verbose = True,
              simX = simDataX,
              simY = simDataY)
```

famospy returns a lot of verbose output, telling the user what's currently happening (Note: The output can be turned on and off by using the option _verbose_). In the beginning, the overall settings are defined and the corresponding directories are created (if they don't exist).

```{python famospystart, eval = FALSE}
Initializing...
Create famos directory...
Algorithm run: 001
Refitting enabled.
Starting algorithm with method 'backward'.
```

In each iteration, the famospy identifies new models to be tested based on the current search method:

```{python iteration, eval = FALSE}
famos iteration # 3 - method: forward
Add parameter p1
Add parameter p3
Add parameter p4
Time passed since start: 0:00:02.010000
```

Each model will be submitted and tested. Since famospy uses futures for evaluation, the search process can be easily parallelised by setting the corresponding future plan. Every model is subsequently evaluated by performing (multiple) optimisation routines based either on the default fitting routine _scipy.optimize.minimize_ or a user-specified fitting routine.

After all models have been evaluated, the algorithm reads in the results and checks, if a better model was found

```{python enditeration, eval = FALSE}
Waiting for jobs to finish ...
Evaluate results ...
Best selection criterion value of this run is 10.0
Switch to method 'backward'
```

The cycle continues until no better model can be found based on the currently used methods. After halting, the results are returned

```{python bestresults, eval = F}
No better model was found. Algorithm terminated.
Time needed: 0:00:07.875000
Best selection criterion value of run 001: 5.71
Number of models tested during this run (might include repeats): 8
The parameters of the best model are: ['p2', 'p5']
Estimated parameter values: {'p1': 0, 'p2': -3.000000004314983, 'p3': 0, 'p4': 0, 'p5': 2.0000000000000044}
Best model binary is: 01001
```

## famospy options in detail

#### initPar

The dictionary _initPar_ is one of two mandatory inputs that need to be specified. It contains the names and initial values of all model parameters, that famospy is supposed to analyse. In our example above, we specified this dictionary as

```{python init.par, eval = FALSE}
#define initial parameter values
inits = dict(p1 = 3, p2 = 4, p3 = -2, p4 = 2, p5 = 0)
```

Depending on the starting model, famospy automatically extracts the corresponding values and uses them for its first iteration only. All following iterations inherit the best values from previous fits.

Additional specifications for the use of the inital parameter vector can be supplied by the options _doNotFit_ and _defaultVal_.

#### fitFn
To allow independence of specific mathematical model structures, the user can specify any cost function. The cost function has to take the complete parameter dictionary as an input (named 'parms') and has to return a selection criterion value. Optionally, the binary model information can be used as well (as an input argument 'binary', see example). If _useOptim = True_, the cost function needs to return a single numeric value, which corresponds to the selection criterion value. However, if _useOptim = False_, the cost function needs to return a dictionary containing in its first entry the selection criterion value (named 'SCV') and in its second entry a list of the fitted parameter values (names 'parameters'). Non-fitted parameters are internally assessed. Here's an example:

```{python fitFn, eval = FALSE}
#template for a cost function
def cost_function(parms, binary, **kwargs):
  ... calculate the SCV from the given parameter values ...
  return SCV

#template for an optimisation function
def opt_function(parms, binary, **kwargs):
  ... use the binary information to extract the to-be-fitted parameters ...
  ... fit the parameters ...
  return dict(SCV = SCV, parameters = list(values_of_fitted_parameters))

```

Due to this flexible structure, famospy is able to tackle many different problems, e.g. modelling approaches like linear regression, ODEs or PDEs.

#### homedir

famospy generates and saves many different files, in order to make results available over time as well as to simultaneously running famospy runs. _homedir_ specifies the folder, in which all results are going to be stored. The default is set to the current working directory.

#### doNotFit

In order to exclude some parameters from the fitting procedures, their names can be specified in the _doNotFit_ option. This allows to test different model restrictions without needing to change either _initPar_ or _fitFn_. For example, if we wanted to exclude the parameter _p4_ from our analysis, we would specify initially

```{python do.not.fit, eval = FALSE}
#define initial parameter values
inits = dict(p1 = 3, p2 = 4, p3 = -2, p4 = 2, p5 = 0)
noFit = list("p4")
```

and pass this option on to famospy. Note that excluded parameters are automatically removed from the initial model, if _initModelType = "random"_ or _initModelType = "global"_ is used. If the user-specified initial model contains an excluded parameter, an error will be returned.

```{python donotfiterror, eval = FALSE}
The specified initial model violates critical conditions or the doNotFit specifications
```
#### method

famospy can use three different methods to search for different models to test: Forward search, backward elimination and swap search. As the algorithm dynamically changes these methods over the course of each iteration, the option _method_ only specifies the starting method.

If the algorithm is able to find a better model, the current method will be used in the next iteration as well (except the swap method, which always uses a forward search next - if it doesn't terminate in that step). If no better model is found, the algorithm will change the method according to the following scheme:

| current method  | previous method | next method |
| :-------------: |:-------------:| :-----:|
| forward      | backward | swap (or terminate)|
| forward      | forward or swap | backward |
| backward | backward |  forward |
| backward | forward |  swap (or terminate) |
| swap | forward or backward |  terminate |

In case the swap method is not used (due to unspecified critical or swap sets), the algorithm will terminate after a succession of an unsuccessful forward and backward search.

#### initModelType

To verify if the famospy results are consistent, it is important to run the algorithm with different starting models. To set the initial model, the user can either use the built-in options _random_ (which generates a random model), _global_ (which uses the complete model as a starting point) or _mostDistant_ (uses the model most dissimilar to all previously tested models). Alternatively, the user can specify a model by supplying a parameter vector containing the names of the initial model.

```{python init.model.type, eval = FALSE}
#Three options for the starting model
initModel1 = "random" # generates a random starting model
initModel2 = "global" # uses all available parameters
initModel3 = "mostDistant" # uses the most dissimilar model
initModel4 <- list("p1", "p4") # a user-specified model
```

In case _random_ or _global_ are chosen, famospy automatically applies critical conditions and removes excluded parameters (see options _criticalParameters_ and _doNotFit_).

#### refit

Before testing a model, famospy checks if this model has been tested before. In case _refit = False_ (default) is specified, the model will not be tested again. If refitting is set to _True_, famospy will try to optimise the model again. If the new run returns a better fit, the old results will be overwritten, otherwise the new run will be discarded.

Refitting makes sense if the model optimisation is dependent on the initial parameter combination (see also _optimRuns_). If a model is reencountered, it might well be that the new parameter set to be tested with is much more appropriate than the previous one, especially if this reencounter happens within the same famospy run.

#### useOptim

The default fitting routine that famospy relies on is the function _scipy.optimize.minimize_. However, by setting _useOptim = False_, the user can use any other fitting routine suitable. The fitting routine then has to be included in the cost function _fitFn_ which needs to return a dictionary containing the current selection value criterion as well as the parameter values used. See the vignettes for an example.

#### optimRuns

Finding the best fit for each model is crucial to guarantee a correct model selection procedure. Often, fitting a model once is enough and repeating the fitting procedure with different initial conditions does not lead to new results. Sometimes, however, one wants to run multiple fits for each model, e.g. if the parameter space is very large. To do so, the user can specify _optimRuns_, which gives the number of fitting attempts. For each optimisation run a different starting condition is used. The first fitting attempt takes the inherited parameter vectors from previous runs, while all following fitting attempts randomly samples parameter vectors to test (see also _randomBorders_).

If multiple optimisation runs are performed, famospy will return the best of these runs.

In each optimisation run fitting in famospy is either performed with the function _scipy.optimize.minimize_, which is repeatedly evaluated until convergence, or a custom optimisation routine, which is evaluated only once.
As the default optimisation method is set to the Nelder-Mead approach, which often tends to not give reliable results if only one optimisation is performed, the optimisation for each fitting attempt is wrapped into a while-loop, in which the fitting procedure is repeatedly halted and restarted (based on the options _controlOptim_), until the relative convergence tolerance in _conTol_ is reached.

The skeleton of the underlying code looks like this:

```{python optim.runs, eval = FALSE}

for i in range(optim.runs):#number of fitting attempts specified by optim.runs
  startParameters = either the inherited or a randomly sampled set (for i > 2)

  if useOptim == True:
    #If useOptim = True, the fitting routine is evaluated in a while loop
    while abs((old.minimize.value - new.minimize.value)/old.minimize.value) < conTol
      ... run minimize with startParameters ...
      startParameters <- new parameters estimated by minimize

  else:
    #If useOptim = False, the custom optimisation routine is evaluated
    #only once in each optimisation run
    ... run custom optimisation with start parameters ...

```

#### defaultVal

Normally, famospy sets the parameters that are not fitted equal to zero. However, this might not be appropriate if, for example, a parameter describes an initial condition or a baseline turnover. Here,  _defaultVal_ allows to specify the value that a parameter assumes, if it is not fitted. _defaultVal_ needs to be given as a dictionary, which can either store numerical values or the name of the parameter from which the value should be inherited. For example

```{python default.val, eval = FALSE}
#define initial parameter values
inits = dict(p1 = 3, p2 = 4, p3 = -2, p4 = 2, p5 = 0)
#set default values
defVal = dict(p1 = 2, p2 = -5, p3 = "p1", p4 = 0, p5 = "p4")

```

Here, the values of _p1_, _p2_, and _p4_ are set to their respective values. However, _p3_ and _p5_ will inherit their values from _p1_ and _p4_, respectively. This feature is useful if two rates describe similar processes and one wants to test if the difference between them is significant enough to warrant the fitting of an additional parameter. Here's a short example

```{python defaultval2, eval = FALSE}
def cost_function(parms):
  x = parms["p1"] + parms["p2"]*x
  y = parms["p3"] + parms["p4"]*x


defVal <- dict(p1 = 0, p2 = 0, p3 = "p1", p4 = "p2")
```

Note that the parameter inheritance cannot be chained, meaning that entries that point to another parameter need a numeric value to access

```{python defaultval3, eval = FALSE}
#INCORRECT use of default.val
defVal <- dict(p1 = 1, p2 = "p1", p3 = "p2", p4 = "p3")
#CORRECT use of default.val
defVal <- dict(p1 = 1, p2 = "p1", p3 = "p1", p4 = "p1")
```

#### swapParameters

The swap search that famospy can perform relies on sets which specify parameters that can be swapped by one another. For example, if we wanted to allow parameters _p1_, _p2_ and _p3_, as well as _p4_ and _p5_ to be replaceable by each other, we would specify:

```{python swaps, eval = FALSE}

swapSet <- list(list("p1", "p2", "p3"), list("p4", "p5"))
```
#### critical.parameters

In some cases, it does not make sense to fit certain submodels of the global model to the data, as they might lack crucial parameters. famospy can incorporate these restrictions by the specification of critical parameter sets. For example, if at least one of the first three parameters need to be present in the model, and all models that don't feature _p4_ are not correct, we can specify:

```{python crits, eval = FALSE}
critSet <- list(list("p1", "p2", "p3"), list("p4"))
```

All critical sets are also automatically used in the swap search.

#### randomBorders

Since the parameters of all _optimRuns_ larger than one are sampled based on a random uniform distribution, it might be important to set the correct sampling intervals. By default, famospy samples parameters with a 100% deviation of the inherited parameter values (for example, if a model contains two parameters, and the currently best values are _p1 = 0.1_ and _p2 = -1000_, the sampled values will lie in the intervals [0,0.2] and [-2000,0], respectively). Alternatively, the user can specify relative or absolute sampling intervals. For relative intervals, a numeric value has to be given for each parameter denoting its relative deviation. For absolute sampling intervals, a matrix containing the lower and upper borders has to be specified. Here's an example:

```{python random.borders, eval = FALSE}
#relative sampling ranges
randomBord1 <- 0.3 # deviates all parameters by 30%
randomBord2 <- list(0.1, 0.5, 0.2) # deviates the parameters by 10%, 50% and 20%, respectively

#absolute sampling ranges
random.bord3 <- list(list(1),list(2)) #uses the interval [1,2] for all parameter samples
random.bord4 <- list(list(0,-10, 0.3), list(5, -9, 0.7)) #uses the intervals [0,5], [-10,-9] and [0.3, 0.7] to sample the respective parameters

#use a function to sample the results
def uni(low, high, size = 1):
  from numpy.random import uniform
  return(uniform(low = low, high = high, size = size))
randomBord5 <- uni #note that in this case, 'low', 'high' and 'size' need to passed to famospy as well, if other values than the default settings should be used

```
#### controlOptim

Specifies the control options used for _scipy.optimize.minimize_ (see _optimRuns_ for more details).

#### conTol

Specifies the relative convergence tolerance and determines when the repeated use _scipy.optimize.minimize_ fits will be terminated (see _optimRuns_ for more details).

#### savePerformance

If true, a plot of the current famospy performance is stored in the folder "FAMoS-Results/Figures/", which will be updated during each iteration.

#### parallelise

Allows to parallelise the fitting of models. Based on the _multiprocessing_ package.

#### logInterval

If futures are used during a famospy run, there will be a message printed every _X_ seconds, informing the user which models fits are still running. _logInterval_ allows to specify the interval of _X_. Default to 10 minutes (600 seconds).

#### verbose

The verbose output of famospy can be turned on and off. If _verbose = False_, only a minimum of information is shown.
